# Great ML conf papers
ML summaries revolutionized
<!--
| student name | qualities |
| ------------ | ---------- |
| John         | ![honest](https://img.shields.io/badge/honest-red) ![hard working](https://img.shields.io/badge/hard%20working-green) ![loyal](https://img.shields.io/badge/loyal-blue) |
| Rebecca      | ![loyal](https://img.shields.io/badge/loyal-blue) ![coy](https://img.shields.io/badge/coy-purple) |
-->


| Conf | Title | Released | Topics | Why you should read the paper? |   Key takeaways from the paper |
| -----|-------|----------| ----| ---- | ------- |
| ACL | [Feature-Adaptive and Data-Scalable In-Context Learning](https://arxiv.org/pdf/2405.10738) | May 2024 | In-context learning | <ul><li>A different way of making In-context learning data-scalable, i.e., how can we incorporate more and more training data for in-context learning when the context window is fixed? <li> How can we "fine-tune" In-context learning to different downstream tasks?</ul> | <ul><li>A simple solution is to use the rest of the training data (which can not be fit into the context) for training a feature adaptor that takes in input the final hidden representation from the transformer and outputs the required output <li> This only works on classification tasks yet. </ul> |
| iCML | [Bottleneck-Minimal Indexing for Generative Document Retrieval](https://arxiv.org/pdf/2405.10974) | May 2024 | IR, Generative IR, ML Theory | <ul><li>To learn more on an emerging segment in IR, GDR or generative document retrieval where the goal is to generate indices (of the documents) given a query (and not the documents themselves! Sounds crazy but research has been on-going for last couple of years) <li>To learn why we need to consider the distribution of query space to obtain optimal indexing for GDR problem</ul> |  <ul><li>Need to read more!</ul> |
| ACL | [RecGPT: Generative Pre-training for Text-based Recommendation](https://arxiv.org/pdf/2405.12715) | May 2024 | Recommendation, GPT | <ul><li>To learn another way of employing GPT for recommendation </ul> | <ul><li>One can use text-representation of user-interaction history to predict both user ratings and next user interaction (aka sequential recommendation) (and that's what this paper presents and calls that model RecGPT). <li>RecGPT is superior than other pre-trained LLM based models like P5 and ChatGPT, as well as conventional recommendation models like MF, however the authors have not yet studied its performance on other recommendation tasks such as Explanation generation, review summarization and direct recommendation (see P5 paper).</ul> |
| ICML | [LeaPformer: Enabling Linear Transformers for Autoregressive and Simultaneous Tasks via Learned Proportions](https://arxiv.org/pdf/2405.13046) | May 2024 | Transformers, Efficiency | <ul><li> To learn another method of linearizing attention computation in transformers, one which does not assume anything about the target or input sequence length and also does not encourage locality bias </ul> | <ul><li>LeaPformer, the linearized Transformer with (Lea)rned (P)ropertions, as they call it, can be thought of as a more general paradigm of linearizing attention computations as it generalizes its preceding works like cosFormer. It's called "Learned Proportions" because the linearized attention scores are reweighed via a score (called proportion), which is learned (rather than being fixed). <li>LeaPformer is neither the most accurate nor the most efficient, but it has the highest rate of accuracy per efficiency (e.g., see Fig 1 in the paper).</ul> |
| ACL | [Surgical Feature-Space Decomposition of LLMs: Why, When and How?](https://arxiv.org/pdf/2405.13039) | May 2024 | Transformers, Compression | <ul><li>You should read this paper to know a simple algorithm to prune transform model layer by layer wherein every layer is pruned via matrix decomposition method </ul> | <ul><li>This paper did help me know that feature-space decomposition (data aware dimension reduction) produces better-performing model than one obtained via weight space decomposition (data agnostic dimension reduction). <li>The paper has not compared against any other pruning algorithm.</ul>

