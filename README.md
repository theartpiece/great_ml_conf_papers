# Great ML conf papers
ML summaries revolutionized
<!--
| student name | qualities |
| ------------ | ---------- |
| John         | ![honest](https://img.shields.io/badge/honest-red) ![hard working](https://img.shields.io/badge/hard%20working-green) ![loyal](https://img.shields.io/badge/loyal-blue) |
| Rebecca      | ![loyal](https://img.shields.io/badge/loyal-blue) ![coy](https://img.shields.io/badge/coy-purple) |
-->


| Conf | Title | Released | Topics | Why you should read the paper? |   Key takeaways from the paper |
| -----|-------|----------| ----| ---- | ------- |
| ACL | [Feature-Adaptive and Data-Scalable In-Context Learning](https://arxiv.org/pdf/2405.10738) | May 2024 | In-context learning | <ul><li>You should read this paper to learn a different way of making In-context learning data-scalable, i.e., how can we incorporate more and more training data for in-context learning when the context window is fixed. <li> You should read this paper to learn how we can "fine-tune" In-context learning to different downstream tasks.</ul> | <ul><li>A simple solution is to use the rest of the training data (which can not be fitted into the context) for training a feature adaptor that takes in input the final hidden representation from the transformer and outputs the required output. <li> This method currently only works on classification tasks. </ul> |
| iCML | [Bottleneck-Minimal Indexing for Generative Document Retrieval](https://arxiv.org/pdf/2405.10974) | May 2024 | IR, Generative IR, ML Theory | <ul><li> You should read this paper to learn to learn more on an emerging segment in IR, GDR or generative document retrieval where the goal is to generate indices (of the documents) given a query (and not the documents themselves! Sounds crazy but seems like research on GDR has been on-going for last couple of years). <li> You should read this paper to learn why we need to consider the distribution of query space to obtain optimal indexing for the GDR problem. </ul> |  <ul><li>Need to read more!</ul> |
| ACL | [RecGPT: Generative Pre-training for Text-based Recommendation](https://arxiv.org/pdf/2405.12715) | May 2024 | Recommendation, GPT | <ul><li> You should read this paper to learn another way of employing GPT for recommendation. </ul> | <ul><li>One can use text-representation of user-interaction history to predict both user ratings and next user interaction (aka sequential recommendation) (and that's what this paper presents and calls that model RecGPT). <li>RecGPT is superior to other pre-trained LLM-based models like P5 and ChatGPT and conventional recommendation models like Matrix Factorization. However, the authors have not yet studied its performance on other recommendation tasks such as Explanation generation, review summarization, and direct recommendation (see P5 paper).</ul> |
| ICML | [LeaPformer: Enabling Linear Transformers for Autoregressive and Simultaneous Tasks via Learned Proportions](https://arxiv.org/pdf/2405.13046) | May 2024 | Transformers, Efficiency | <ul><li>  You should read this paper to learn another method of linearizing attention computation in transformers, one which does not assume anything about the target or input sequence length and also does not encourage locality bias. </ul> | <ul><li>LeaPformer, the linearized Transformer with (Lea)rned (P)ropertions, as they call it, can be thought of as a more general paradigm of linearizing attention computations as it generalizes its preceding works like cosFormer. It's called "Learned Proportions" because the linearized attention scores are reweighed via a score (called proportion), which is learned (rather than being fixed). <li>LeaPformer is neither the most accurate nor the most efficient but has the highest accuracy per efficiency ratio (e.g., see Fig 1 in the paper).</ul> |
| ACL | [Surgical Feature-Space Decomposition of LLMs: Why, When and How?](https://arxiv.org/pdf/2405.13039) | May 2024 | Transformers, Compression | <ul><li>You should read this paper to know a simple algorithm to prune transform model layer by layer wherein every layer is pruned via matrix decomposition method. </ul> | <ul><li>This paper did help me know that feature-space decomposition (data aware dimension reduction) produces a better-performing model than one obtained via weight space decomposition (data agnostic dimension reduction). <li>The paper has not compared against any other pruning algorithm.</ul> |
| ICML | [Autoformalizing Euclidean Geometry](https://arxiv.org/pdf/2405.17216) | May 2024 | Transformers, ICL, Automatic Theorem Provers | <ul><li> You should read this paper to learn about how LLMs can be used to formalize proofs written in text. <li> You should read this paper to learn why verifying proofs generated by LLMs is difficult as it involves filling the gaps in those proofs (these gaps are nothing but lemmas that are trivial for a human but nonetheless difficult for a simple proof verification engine). Similarly, the proof verification engine also needs to be enhanced to be able to prove the equivalence between the generated output and the ground truth, which we can call the proof equivalence engine and which they call E3. </ul> | <ul><li>The main artifact of the paper is the proof equivalence engine E3, which sometimes falsely indicates non-equivalence (about 15%), which makes sense because this engine is heuristics driven. <li> GPT-4 can produce exact correct proofs about 20% times. </ul> 

